{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "poza-MusicVAE",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN0Xomg2wSniR3EgTOco16W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sunjii/MusicVAE/blob/main/poza_MusicVAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 과제 상세안내\n",
        "\n",
        "- 해당 과제는 `MusicVAE`를 구현하는 것입니다.\n",
        "\n",
        "그 중에서도 Groove MIDI Dataset을 이용하여 4마디에 해당하는 드럼 샘플을 뽑아내는 과정을 수행하시면 됩니다.\n",
        "\n",
        "- 깃헙 코드: https://github.com/magenta/magenta/tree/master/magenta/models/music_vae\n",
        "\n",
        "- 관련 논문: https://arxiv.org/pdf/1803.05428.pdf\n",
        "\n",
        "- 관련 데이터: https://magenta.tensorflow.org/datasets/groove\n",
        "\n",
        "미디라는 데이터는 음악이 어떻게 연주되어야 하는지 나타내는 악보와 비슷한 개념으로 이해하시면 됩니다. 어느 시점에 어떤 악기를 연주하여야 하는지가 나와있는 데이터이며 .midi의 확장자명을 가지고 있습니다.\n",
        "\n",
        "설명드린 코드에서는 해당 미디를 학습에 이용하기 위하여 벡터로 변환하는 전처리 과정이 필요합니다.\n",
        "\n",
        "이러한 전처리 과정을 거치면 tfrecord형식으로 저장되게 되며 이것을 학습하는 형태입니다.\n",
        "\n",
        "- 과제는 어떠한 방식으로 모델을 다루는지를 보기 위함이며, 과제를 100% 완수하지 못하셔도 됩니다."
      ],
      "metadata": {
        "id": "O7NVA9_8pR6z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wwv2UXH0qY_q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 사전 지식 탐색 및 정리\n",
        "\n",
        "## VAE\n",
        "- VAE는 latent representation에 효과적인 모델로 입증되었음\n",
        "- AE와 형태면에서 비슷함. Bottleneck 구조를 통해 벡터를 압축하여 latent code를 생성. \n",
        "  - latent space를 통해 입력 데이터간의 유사성과 차이를 통해 학습\n",
        "  - VAE는 AE와는 달리 Generative한 모델임.\n",
        "  - 생성에 강점을 보이는 이유는 latent space가 연속되게 생성되기 때문. 반면 AE는 이산적인 분포를 가지기 때문에 데이터 재구성 단계에서 저품질의 데이터가 생기는 원인이 됨.\n",
        "  - VAE는 KL-divergence를 loss 함수로 사용함\n",
        "- KL-divergence를 통해서 VAE의 latent space 분포가 `표준 정규 분포`에 가깝도록 학습하게 됨\n",
        "\n",
        ">참고링크: https://ratsgo.github.io/generative%20model/2017/12/19/vi/\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## VAE 한계점\n",
        "\n",
        "- sequential data에 한계점이 존재함.\n",
        "- recurrent VAE 모델들은 long-term sequence에 약함\n",
        "- 논문에서는 이러한 한계점을 극복하기 위해 `hierachical decoder`를 제안함\n",
        "  - 첫번째 output embedding이 input의 sub-sequence로 쓰임\n",
        "  - 각 embedding들은 독립적으로 sub-sequence들을 생성함\n",
        "- 이러한 구조는 모델이 latent code를 utilize하고 `posterior collapse` 문제를 회피하는데 도움이 됨\n",
        "\n",
        "  - posterior collapse: 시퀀스 모델을 VAE형태로 표현하면 global latent z를 이용하여 다양한 속성의 시퀀스를 생성할 수 있다. 하지만 이 때 decoder가 encoder의 condition을 무시하고 시퀀스를 생성하는 현상을 말함.\n",
        "\n",
        "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fqd1lx%2FbtqXMfe0scF%2FYRdkqrZhcInlZ2yPRchOzK%2Fimg.png)\n",
        "\n",
        "  - collapse 발생하는 이유\n",
        "    - decoder가 latent z 없이 과거 데이터만으로 충분히 생성 가능한 경우\n",
        "    - 다양한 latent z가 존재할 수 있음\n",
        "    - VAE가 local information을 선호하는 경향이 있음\n",
        "    - 학습 초기 encoder가 z를 잘 표현하지 못 해서 등등..\n",
        "    - 참고링크: https://stopspoon.tistory.com/63"
      ],
      "metadata": {
        "id": "DGSUTSA-lVWx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model\n",
        "- RNN은 recurrent한 모델이기 때문에 latent code를 무시할 수 있음\n",
        "- 전체 시퀀스를 단일 latent vector로 압축하게 되는데 따라서 long-term 시퀀스인 경우 정보의 손실일 일어날 수 있음\n",
        "- 이 같은 문제를 해결하기 위해 `hierarchical RNN`을 디코더에 적용\n",
        "\n",
        "![](https://i.imgur.com/y397wfb.png)\n",
        "\n",
        "## Bidirectional Encoder\n",
        "- 2 layer 양방향 LSTM 사용\n",
        "- input 시퀀스 x로 첫번째 state vector $ \\overrightarrow{h_{T}} $ 와 $ \\overleftarrow{h_{T}} $ 를 두번째 bi-direction LSTM layer에서 얻는다.\n",
        "- 이는 concat되어 $h_{T}$를 얻고 2개의 fully connectd layer로 들어간다.\n",
        "\n",
        "$$ \\mu = W_{h\\mu}h_{T} + b_{\\mu} $$\n",
        "\n",
        "$$ σ = log(exp(W_{hσ} + b_{σ}) + 1) $$\n",
        "\n",
        "$W_{hμ}, W_{hσ}$는 가중치 행렬이고, $b_μ, b_σ$는 bias 벡터이다. 논문에서는 2048개의 layer와 512의 latent 차원을 사용한다. 일반적인 VAE처럼 뮤와 시그마를 통해서 latent 분포를 표현할 수 있다. 양방향 recurrent encoder는 이상적으로 input 시퀀스의 long-term context를 잘 표현할 수 있다고 함.\n",
        "\n",
        "## Hierarchical Decoder\n",
        "- 단층 RNN을 사용한 경우 -> 매우 떨어지는 샘플링 및 재구성 성능을 보임\n",
        "  - 출력 시퀀스가 생성되면서 latent state의 영향력이 점차 소실되기 때문 (vanishing)\n",
        "- 이러한 문제를 해결하기 위해 계층적 RNN 구조를 디코더에 적용함\n",
        "- input 시퀀스 x를 중복되지 않는 U개의 `sub sequence` $y_u$로 나누게 되면 다음처럼 표현 할 수 있음. 여기서 $i_u$는 endpoint임\n",
        "\n",
        "$$ y_u = \\{ x_{i_u}, x_{i_u+1}, x_{i_u+2}, ..., x_{i_u+1} -1 \\} $$\n",
        "\n",
        "$$ →X = \\{ y_1, y_2, ..., y_U \\} $$\n",
        "\n",
        "- 위 식에서 $i_{U+1} = T$로 정의함. 이후 latent vector z는 fully-connetced layer를 통과하며 tanh 활성 함수를 거쳐 conductor RNN의 초기 state를 생성함.\n",
        "\n",
        "- 논문에서는 Conductor를 위해서 2개층의 양방향 LSTM (hidden state size 1024, 512 output dimension)을 사용함.\n",
        "\n",
        "### Conductor\n",
        "\n",
        "- conductor가 임베딩 벡터 c를 생성하면 각 벡터는 독립적으로 fully-connected layer를 지나면서 tanh 활성함수를 통해 RNN decoder의 최종 바닥층의 초기 state를 생성함.\n",
        "- RNN 디코더는 softmax 출력층을 거치면서 auto-regressive하게 시퀀스 분포를 생성함. 여기서 디코더는 서브시퀀스 $y_{u}$에 해당하는 분포를 생성함.\n",
        "- 디코더의 바닥층에서는 매 step마다 conductor의 임베딩 $c_u$와 이전 step의 출력 토큰을 결합하여 input으로 사용함.\n",
        "  - 이러한 계층적 구조때문에 각 서브시퀀스 $y_u$는 연결된 conductor에서 생성된 $c_u$를 통해서만 영향을 받음\n",
        "- 논문에서는 2 layer LSTM과 각 layer마다 1024 units의 decoder RNN을 사용함.\n",
        "\n",
        "### 결과\n",
        "\n",
        "- 논문에서는 실험을 통해 decoder의 scope를 제한하는 것이 latent code를 사용하는 long-term 구조 모델에 중요하다는 사실을 발견함\n",
        "- 각 서브시퀀스의 끝에서 decoder의 state가 다시 conductor로 들어가는 `auto regressive conductor`에서는 성능이 좋지 않았음.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1XfSqa69qb0y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation\n",
        "\n",
        "## Data \n",
        "\n",
        "MIDI 데이터를 학습에 사용하기 위해서 벡터로 변환하는 전처리 과정이 필요하다.\n",
        "\n",
        "[magenta](https://github.com/magenta/magenta/tree/main/magenta/models/music_vae) 를 이용하여 .mid, .midi 파일을 tfrecord 포멧으로 변환시킬 수 있다. 이후 학습에 사용하면 된다!\n",
        "\n",
        "* tfrecord: 바이너리 데이터 포멧으로 serial 데이터를 읽는데 특화되어 있다. [참고링크](https://bcho.tistory.com/1190)"
      ],
      "metadata": {
        "id": "ObGCN5xZNArV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 구글 드라이브 연결\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "# data dir\n",
        "DATA_DIR = '/content/drive/MyDrive/musicVAE/data'\n",
        "OUT_DIR = '/content/drive/MyDrive/musicVAE/data_out'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TNYdPpZlVGN",
        "outputId": "b3e47e1a-713a-4d70-8755-76ec21209838"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 확인\n",
        "\n",
        "간단하게 tfds에 있는 groove 데이터를 가져와서 사용하겠습니다. 4마디 드럽 샘플을 뽑아내기 위해 4마디 데이터셋인 groove/4bar-midionly를 가져옵니다."
      ],
      "metadata": {
        "id": "FCTvxdFhGGuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "groove = tfds.builder(\n",
        "    name=\"groove/4bar-midionly\", # 4마디 드럼 데이터셋을 불러옵니다.\n",
        "    )\n",
        "print(groove.info)\n",
        "print(groove.info.features['midi'])\n",
        "print(groove.info.features['drummer'].names)\n",
        "print(groove.info.features['style']['primary'].names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZB7TZtEDgRVO",
        "outputId": "ff29d273-216e-4c31-b48d-2da38a22d77f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tfds.core.DatasetInfo(\n",
            "    name='groove',\n",
            "    version=2.0.1,\n",
            "    description='The Groove MIDI Dataset (GMD) is composed of 13.6 hours of aligned MIDI and\n",
            "(synthesized) audio of human-performed, tempo-aligned expressive drumming\n",
            "captured on a Roland TD-11 V-Drum electronic drum kit.',\n",
            "    homepage='https://g.co/magenta/groove-dataset',\n",
            "    features=FeaturesDict({\n",
            "        'bpm': tf.int32,\n",
            "        'drummer': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
            "        'id': tf.string,\n",
            "        'midi': tf.string,\n",
            "        'style': FeaturesDict({\n",
            "            'primary': ClassLabel(shape=(), dtype=tf.int64, num_classes=18),\n",
            "            'secondary': tf.string,\n",
            "        }),\n",
            "        'time_signature': ClassLabel(shape=(), dtype=tf.int64, num_classes=5),\n",
            "        'type': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
            "    }),\n",
            "    total_num_examples=21415,\n",
            "    splits={\n",
            "        'test': 2033,\n",
            "        'train': 17261,\n",
            "        'validation': 2121,\n",
            "    },\n",
            "    supervised_keys=None,\n",
            "    citation=\"\"\"@inproceedings{groove2019,\n",
            "        Author = {Jon Gillick and Adam Roberts and Jesse Engel and Douglas Eck and David Bamman},\n",
            "        Title = {Learning to Groove with Inverse Sequence Transformations},\n",
            "        Booktitle\t= {International Conference on Machine Learning (ICML)}\n",
            "        Year = {2019},\n",
            "    }\"\"\",\n",
            "    redistribution_info=,\n",
            ")\n",
            "\n",
            "Tensor(shape=(), dtype=tf.string)\n",
            "['drummer1', 'drummer2', 'drummer3', 'drummer4', 'drummer5', 'drummer6', 'drummer7', 'drummer8', 'drummer9', 'drummer10']\n",
            "['afrobeat', 'afrocuban', 'blues', 'country', 'dance', 'funk', 'gospel', 'highlife', 'hiphop', 'jazz', 'latin', 'middleeastern', 'neworleans', 'pop', 'punk', 'reggae', 'rock', 'soul']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.ops.numpy_ops import np_config\n",
        "np_config.enable_numpy_behavior()"
      ],
      "metadata": {
        "id": "13Y94NDBLhKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the full GMD with MIDI only (no audio) as a tf.data.Dataset\n",
        "train_dataset, val_dataset, test_dataset = tfds.load(\n",
        "    name=\"groove/4bar-midionly\", # 4마디 드럼 데이터셋을 불러옵니다.\n",
        "    split=(tfds.Split.TRAIN, tfds.Split.VALIDATION, tfds.Split.TEST),\n",
        "    try_gcs=True,\n",
        "    )\n",
        "\n",
        "\n",
        "# Build your input pipeline\n",
        "train_dataset = train_dataset.shuffle(1024).batch(32).prefetch(\n",
        "    tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "val_dataset = val_dataset.shuffle(1024).batch(32).prefetch(\n",
        "    tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "test_dataset = test_dataset.shuffle(1024).batch(32).prefetch(\n",
        "    tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "print(len(train_dataset), len(val_dataset), len(test_dataset))\n",
        "for features in train_dataset.take(1):\n",
        "  # Access the features you are interested in\n",
        "  midi = features[\"midi\"]\n",
        "  print('=============')\n",
        "  print(midi.shape)\n",
        "  print(midi)\n"
      ],
      "metadata": {
        "id": "WtlSu1Sc8Psk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VAE\n",
        "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fqd1lx%2FbtqXMfe0scF%2FYRdkqrZhcInlZ2yPRchOzK%2Fimg.png)\n",
        "\n",
        "- 인코더\n",
        "\n",
        "양방향 LSTM encoder 사용\n",
        "\n",
        "- 디코더\n",
        "\n",
        "단방향 LSTM decoder 사용\n",
        "\n",
        "## Hierarchical Decoder\n",
        "\n",
        "- Conductor\n",
        "\n",
        "- 논문에서 진행한 방식에 따라 모델 구축 및 변수 설정을 하겠습니다.\n",
        "- 변수 설정\n",
        "  - 2-bar data -> T = 32\n",
        "  - 16-bar data -> T = 256\n",
        "  - 4-bar data -> T = 64가 됩니다.\n",
        "  - U = 16, 전체 입력 X는 16개의 서브시퀀스로 나뉘게 됩니다.\n",
        "  - Adam 을 사용했으며, lr_rate는 1e-3 ~ 1e-5 이며 exponential decay rate는 0.9999를 사용함\n",
        "  - batch_size는 512\n",
        "  - loss는 cross-entropy 사용\n",
        "  - 2-bar model and teacher forcing for 16-bar model\n",
        "\n",
        "- 기타 파라미터\n",
        "  - max_seq_len = 64 # 4마디 길이\n",
        "  - z_size = 512 # latent vector size\n",
        "  - slice_bars = 4 # 4마디"
      ],
      "metadata": {
        "id": "hd2SvvfEBVjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "#def kl_divergence(p, q):\n",
        "  #return p*tf.math.log(p/q) + (1-p)*tf.math.log((1-p) / (1-q))\n",
        "\n",
        "def kl_divergence(p, q):\n",
        "    return np.sum(np.where(p != 0, p * np.log(p / q), 0))\n",
        "\n",
        "\n",
        "def sampling(inputs):\n",
        "  \"\"\"\n",
        "  return\n",
        "    z: latent vector\n",
        "  \"\"\"\n",
        "  z_mean, z_log_var = inputs\n",
        "  eps = tf.random.normal(tf.shape(z_log_var), dtype=tf.float32, mean=0., stddev=1.0, name='epsilon')\n",
        "  z = z_mean + tf.exp(z_log_var / 2) * eps\n",
        "  \n",
        "  return z\n",
        "\n"
      ],
      "metadata": {
        "id": "G6wUE1-gu8XS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_dim = 512\n",
        "latent_dim = 2\n",
        "num_feat = 1 ## data feature?\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "yOhQIOlnu8Uw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encoder\n",
        "inputs = keras.layers.Input(shape=(num_feat, ), name='midi')\n",
        "x = keras.layers.Dense(hidden_dim, activation='tanh')(inputs)\n",
        "z_mean = keras.layers.Dense(latent_dim, name='z_mean')(x)\n",
        "z_log_var = keras.layers.Dense(latent_dim, name='z_log_var')(x)"
      ],
      "metadata": {
        "id": "ESunAKn6u8SD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z = keras.layers.Lambda(sampling, name='z')([z_mean, z_log_var])\n",
        "\n",
        "encoder = keras.Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
        "encoder.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVCqL6Tzu8O2",
        "outputId": "7012e6ef-fc0c-4584-cc23-c1b06c90ada9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"encoder\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " midi (InputLayer)              [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_32 (Dense)               (None, 512)          1024        ['midi[0][0]']                   \n",
            "                                                                                                  \n",
            " z_mean (Dense)                 (None, 2)            1026        ['dense_32[0][0]']               \n",
            "                                                                                                  \n",
            " z_log_var (Dense)              (None, 2)            1026        ['dense_32[0][0]']               \n",
            "                                                                                                  \n",
            " z (Lambda)                     (None, 2)            0           ['z_mean[0][0]',                 \n",
            "                                                                  'z_log_var[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3,076\n",
            "Trainable params: 3,076\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# decoder \n",
        "\n",
        "latent_inputs = keras.layers.Input(shape=(latent_dim, ), name='z_sampling')\n",
        "x = keras.layers.Dense(hidden_dim, activation='tanh')(latent_inputs)\n",
        "outputs = keras.layers.Dense(num_feat, activation='tanh')(x)\n",
        "\n",
        "decoder = keras.Model(latent_inputs, outputs, name='decoder')\n",
        "decoder.summary()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pm0I4Y6gu8MJ",
        "outputId": "efb394e4-714d-4b71-dd0f-0afe85969186"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"decoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " z_sampling (InputLayer)     [(None, 2)]               0         \n",
            "                                                                 \n",
            " dense_33 (Dense)            (None, 512)               1536      \n",
            "                                                                 \n",
            " dense_34 (Dense)            (None, 1)                 513       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,049\n",
            "Trainable params: 2,049\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = decoder(encoder(inputs)[2])\n",
        "\n",
        "vae = keras.Model(inputs, outputs, name='vae')\n",
        "\n",
        "loss = tf.losses.binary_crossentropy(inputs, outputs)\n",
        "kl_loss = tf.losses.kl_divergence(inputs, outputs)\n",
        "vae.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVGyP8Z8u8JG",
        "outputId": "555b743d-bebe-4df2-b49c-4b3e1e8cef55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"vae\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " midi (InputLayer)           [(None, 1)]               0         \n",
            "                                                                 \n",
            " encoder (Functional)        [(None, 2),               3076      \n",
            "                              (None, 2),                         \n",
            "                              (None, 2)]                         \n",
            "                                                                 \n",
            " decoder (Functional)        (None, 1)                 2049      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,125\n",
            "Trainable params: 5,125\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "vae.add_loss(kl_loss)\n",
        "vae.compile(optimizer='adam')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "ZC6_-nMEzpNr",
        "outputId": "e02b44d0-7eeb-43ee-8c69-6055ae4f4d87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-246-95cce6bb770e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkl_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36madd_loss\u001b[0;34m(self, losses, **kwargs)\u001b[0m\n\u001b[1;32m   1590\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msymbolic_loss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msymbolic_losses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1591\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_is_graph_network'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1592\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_network_add_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1593\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m         \u001b[0;31m# Possible a loss was added in a Layer's `build`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m_graph_network_add_loss\u001b[0;34m(self, symbolic_loss)\u001b[0m\n\u001b[1;32m    906\u001b[0m     \u001b[0mnew_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_loss_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minbound_nodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m     \u001b[0mnew_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_loss_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insert_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_nodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_graph_network_add_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m_insert_layers\u001b[0;34m(self, layers, relevant_nodes)\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m     \u001b[0;31m# Insert layers and update other layer attrs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m     \u001b[0mlayer_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_tracked_trackables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m     \u001b[0mdeferred_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/data_structures.py\u001b[0m in \u001b[0;36m__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unhashable type: 'DictWrapper'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'DictWrapper'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vae.fit(train_dataset, epochs=1, batch_size=batch_size, validation_data=val_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 750
        },
        "id": "OPNCAs5VzpLC",
        "outputId": "9b7acac4-17df-4225-ce8b-0006729199d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:559: UserWarning: Input dict contained keys ['bpm', 'drummer', 'id', 'style', 'time_signature', 'type'] which did not match any model input. They will be ignored by the model.\n",
            "  inputs = self._flatten_to_reference_inputs(inputs)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "StagingError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mStagingError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-247-f9332289acc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mStagingError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 859, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/node.py\", line 165, in map_arguments\n        flat_arguments[kt_index] = tensor_dict[kt_id].pop()\n\n    IndexError: Exception encountered when calling layer \"vae\" (type Functional).\n    \n    pop from empty list\n    \n    Call arguments received:\n      • inputs={'bpm': 'tf.Tensor(shape=(None,), dtype=int32)', 'drummer': 'tf.Tensor(shape=(None,), dtype=int64)', 'id': 'tf.Tensor(shape=(None,), dtype=string)', 'midi': 'tf.Tensor(shape=(None,), dtype=string)', 'style': {'primary': 'tf.Tensor(shape=(None,), dtype=int64)', 'secondary': 'tf.Tensor(shape=(None,), dtype=string)'}, 'time_signature': 'tf.Tensor(shape=(None,), dtype=int64)', 'type': 'tf.Tensor(shape=(None,), dtype=int64)'}\n      • training=True\n      • mask=None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PyTorch\n",
        "\n",
        "![](https://gaussian37.github.io/assets/img/dl/concept/vae/0.png)\n",
        "![](https://i.imgur.com/y397wfb.png)\n",
        "\n",
        "파이토치로 구현\n",
        "\n",
        "- Encoder\n",
        "  - mu:\n",
        "  - sigma:\n",
        "  - z:\n",
        "- Decoder\n",
        "  - x'\n",
        "\n",
        "- Condutor\n",
        "  - \n"
      ],
      "metadata": {
        "id": "6ML3Nu-eSMfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "nIXo-QJhWPHg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "VAE의 가장 핵심은 인코더-디코더 구조와 mu, sigma이다.\n",
        "\n",
        "먼저 인코더는 데이터를 받아와서 mu와 sigma를 출력해야 한다. 인코더는 데이터를 입력받아 hidden_size 만큼 출력하고, 이를 받아서 다시 latent_size 만큼의 mu와 sigma를 출력한다\n",
        "\n",
        "이 작업은 encode 함수를 통해 이루어진다.\n",
        "\n",
        "그 다음에는 reparameterization 과정이 필요하다. 만약 인코더의 결과물을 바로 디코더로 넘긴다면 어떻게 될까? 당연히 한 가지 값만 가지므로 그에대한 디코더 값 역시 한 값만 나오게 된다. 즉, 어떤 입력에 대해서 무조건 똑같은 output이 나오게 된다.\n",
        "\n",
        "하지만 VAE는 Generative 모델이다. 이는 곧 data의 분포를 가지고서 기존에 없는 새로운 data를 생성하고자 하는 것이다. 따라서 data의 분포를 샘플링 하되 reparmeterization을 적용한다.\n",
        "\n",
        "마지막으로 디코더 과정이다. self.decode(z)의 결과는 입력 데이터의 사이즈와 동일한 차원을 가지면서 sigmoid를 통과하므로 0~1 사이의 값을 갖게 된다. 이는 곧 input으로 투입한 데이터를 VAE를 거쳐 복원한 것이라고 볼 수 있다...\n",
        "\n",
        "\n",
        "출처: https://github.com/PeterKim1/paper_code_review"
      ],
      "metadata": {
        "id": "MS6tGm0pj8MT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VAE Model\n",
        "\n",
        "class VAE(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, latent_size):\n",
        "    super(VAE, self).__init__()\n",
        "\n",
        "    # encode\n",
        "    self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "    self.fc21 = nn.Linear(hidden_size, latent_size) # mu\n",
        "    self.fc22 = nn.Linear(hidden_size, latent_size) # sigma\n",
        "\n",
        "    # decode\n",
        "    self.fc3 = nn.Linear(latent_size, hidden_size)\n",
        "    self.fc4 = nn.Linear(hidden_size, input_size)\n",
        "\n",
        "  def encode(self, x):\n",
        "    h = F.relu(self.fc1(x)) # hidden_\n",
        "    return self.fc21(h), self.fc22(h) # mu, sigma\n",
        "\n",
        "  def reparam(self, mu, logvar):\n",
        "    # logvar? -> 표준편차 값이 음수가 되지 않도록 encoder의 결과값을 logs^2 함 (로그 분산)\n",
        "    std = torch.exp(0.5 * logvar)\n",
        "    eps = torch.rand_like(std) # 정규분포에서 숫자를 뽑게 됨\n",
        "    # z = mu + std * eps  ### z 는 latent vector다\n",
        "    return mu + std * eps \n",
        "\n",
        "  def decode(self, z):\n",
        "    h = F.relu(self.fc3(z))\n",
        "    return torch.sigmoid(self.fc4(h)) # input과 동일한 차원을 가지며, 0~1 사이의 값\n",
        "\n",
        "  def forward(self, x):\n",
        "    mu, logvar = self.encode(x.view(-1, input_size)) # \n",
        "    z = self.reparam(mu, logvar)\n",
        "    return self.decode(z), mu, logvar\n",
        "\n",
        "\n",
        "def loss_function(reconstruction_err, x, mu, logvar):\n",
        "  BCE = F.binary_cross_entropy(reconstruction_err, x.view(-1, input_size), reduction='sum') # Bernoulli cross entrophy\n",
        "  KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) # variational lower bound. 쿨백 라이블러\n",
        "  \n",
        "  # BCE + KLD\n",
        "  return BCE, KLD\n"
      ],
      "metadata": {
        "id": "m60kWYgeebrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch, model, train_loader, optimizer):\n",
        "  model.train()\n",
        "  train_loss = 0\n",
        "  for bat_idx, (data, _) in enumerate(train_loader): # 라벨이 필요가 없음 _\n",
        "    data = data.to(device)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    recon_batch, mu, logvar = model(data)\n",
        "\n",
        "    BCE, KLD = loss_function(recon_batch, data, mu, logvar)\n",
        "    loss = BCE + KLD\n",
        "\n",
        "    loss.backward()\n",
        "    train_loss += loss.item()\n",
        "    optimizer.step()\n",
        "\n",
        "    if bat_idx % 100 == 0:\n",
        "      print('Train epo: {} [{}/{} ({:.0f}%)]\\t Loss: {:.6f}'.format(\n",
        "          epoch, bat_idx * len(data), len(train_loader.dataset),\n",
        "          100. * bat_idx / len(train_loader),\n",
        "          loss.item() / len(data)\n",
        "          ))\n",
        "    \n",
        "    print('===========> Epoch: {} Avg loss: {:.4f}'.format(\n",
        "        epoch, train_loss / len(train_loader.dataset)\n",
        "    ))"
      ],
      "metadata": {
        "id": "RBs6cHgSecTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VAE_model = VAE(input_size, 512, 2).to(device)\n",
        "optimizer = optim.Adam(VAE_model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "zbxpdl37ecWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GUfxMmYuecQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZsuH6F6xmQ-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dHePilsrmQxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "A6yQubRbmQwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, latent_dim):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_hidden = 2*2\n",
        "    self.final_size = self.num_hidden * hidden_size\n",
        "\n",
        "    self.lstm = nn.LSTM(batch_first=True,\n",
        "                        input_size=input_size,\n",
        "                        hidden_size=hidden_size,\n",
        "                        num_layers=2,\n",
        "                        bidirectional=True)\n",
        "    \n",
        "    self.mu = nn.Linear(self.final_size, latent_dim)\n",
        "    self.sigma = nn.Linear(self.final_size, latent_dim)\n",
        "    self.z = nn.Linear(self.final_size, )\n",
        "  \n",
        "  def encode(self, x):\n",
        "    x, (hidden, cells) = self.lstm(x)\n",
        "    hidden = hidden.transpose(0, 1).reshape(-1, self.final_size)\n",
        "\n",
        "    mu = \n",
        "    sigma = \n",
        "\n",
        "    return z, mu, sigma\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "id": "dJP8WNDjzpIQ",
        "outputId": "7b245daa-1c47-4dd7-9639-70e33dff9105"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-56-e6b9ad53230a>\"\u001b[0;36m, line \u001b[0;32m22\u001b[0m\n\u001b[0;31m    mu =\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Conductor(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super(Conductor, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_hidden = 2\n",
        "    self.bar = 4\n",
        "\n",
        "    self.lstm = nn.LSTM(batch_first=True,\n",
        "                        input_size=input_size,\n",
        "                        hidden_size=hidden_size,\n",
        "                        num_layers=1,)\n",
        "    \n",
        "    def forward(self, x):\n",
        "      pass\n",
        "      return a, b\n",
        "    \n",
        "\n"
      ],
      "metadata": {
        "id": "uHT-VPyJWR0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size=hidden_size\n",
        "    self.output_size=output_size\n",
        "    self.lstm = nn.LSTM(batch_first=True,\n",
        "                        input_size=input_size+output_size,\n",
        "                        hidden_size=hidden_size,\n",
        "                        bidirectional=False)\n",
        "    self.logits = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def decode(self, x, hidden, cells):\n",
        "      x, (hidden, cells) = self.lstm(x, (hidden, cells))\n",
        "      logits = self.logits(x)\n",
        "      prob = nn.Softmax()(logits)\n",
        "      output = torch.argmax(prob)\n",
        "      \n",
        "      return output, prob, hidden, cells"
      ],
      "metadata": {
        "id": "6LnDrnVDzpFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# magenta 활용\n",
        "\n",
        "magenta 라이브러리를 이용하여 다른 데이터셋으로 musicVAE를 학습할 수 있다.\n",
        "\n",
        "[참고링크](https://github.com/magenta/magenta/tree/main/magenta/models/music_vae)"
      ],
      "metadata": {
        "id": "7zAZD3rHA0UR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q magenta"
      ],
      "metadata": {
        "id": "3pM6jtsrBEjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://raw.githubusercontent.com/tensorflow/magenta/main/magenta/tools/magenta-install.sh > /tmp/magenta-install.sh\n",
        "!bash /tmp/magenta-install.sh"
      ],
      "metadata": {
        "id": "KmA-vslkBDGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MyDrive/musicVAE/data 폴더에 groove MIDI [데이터셋](https://magenta.tensorflow.org/datasets/groove#midi-data)을 압축 해제하고 사용합니다."
      ],
      "metadata": {
        "id": "gS5KBpD6BnK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!convert_dir_to_note_sequences --input_dir='/content/drive/MyDrive/musicVAE/data' --output_file=notesequences.tfrecord --recursive"
      ],
      "metadata": {
        "id": "0BeZPJm3BhcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!music_vae_train --config=groovae_4bar --run_dir=music_vae/ --num_steps=2000 --mode=train --examples_path=notesequences.tfrecord \\\n",
        "--hparams=max_seq_len=64, z_size=512, batch_size=32, learning_rate=0.0005"
      ],
      "metadata": {
        "id": "bF9jggW5A0Lu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!music_vae_generate --config=groovae_4bar --checkpoint_file=music_vae/train --mode=sample --num_output=5 --output_dir=music_vae/generated"
      ],
      "metadata": {
        "id": "F70LwdooA0Dy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}